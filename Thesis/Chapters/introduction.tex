\chapter{Introduction}
% General classification (daunting)
% Neural networks
% DNN
% Adversarial attacks
% Defenses
% Bypass
% Overview
One of the most basic tasks in \gls{ai} or more specifically \gls{ml} is the classification of data. This task is usually performed by a classifier or model based on features present in this data. Some examples of classification tasks are character recognition \cite{mnist}, spam detection \cite{spambase} and face recognition \cite{face_recognition_survey}. These tasks are generally very daunting for humans causing great interest in improving the accuracies of these classifiers.\\

The first classifiers made use of algorithms such as nearest neighbors \cite{nearest_neighor}, decision trees \cite{decision_tree} and \glspl{svm} \cite{svm}. More recent classifiers are based on neural networks, which in turn are inspired by the human brain. These networks come in a great variety of forms and show near-human performance on some very specific tasks \cite{alpha_go_google}. This performance improvement caused even more interest in the applications of \gls{ai} and \gls{ml}. Forbes \cite{forbes} recently predicted that in ten years \gls{ai} will be present in all areas of our society. The next decade is deemed as the \textit{"most promising era in technology innovation"} \cite{forbes}.\\

The prevalence of \gls{ml} in the near future does not only bring opportunities. It also poses some threats that may not be obvious at first glance. A spam email passing through an \gls{ai}-based spam filter might not be the end of the world while mispredicting some characters from an image can have a bigger impact depending on the context. False predictions in the medical domain might be even more severe, but they could be overturned by a doctor. Self-driving cars not detecting traffic signs could put several lives at risk. Depending on the situation in which the classifier is deployed, some threats can be more dangerous than others.\\

All previous examples of threats are due to the inaccuracies of the classifier itself. There are also some threats caused by the inherent nature of neural networks. In 2014, it was shown that neural networks are prone to adversarial examples \cite{szegedy2014intriguing}. An adversarial example consists of a correctly classified data instance and a small perturbation. This perturbation causes the neural network to misclassify the instance. This allows malicious users to create images that are seemingly identical while being classified differently. This poses a great threat to the security of \gls{ai}.\\

The process of creating an adversarial example is called an adversarial attack. Different types of attacks exist depending on the information available to the attacker. White-box attacks require complete knowledge of the classifier under attack, while black-box attacks only require the output(s) of the model. All attacks require sending one or more queries to the classifier.\\

Ever since the discovery of adversarial examples, an arms race between adversarial attack and defense researchers has taken place. The landscape of adversarial defenses includes a wide variety of defensive mechanisms, such as adversarial training \cite{FGSM}, defensive distillation \cite{defensive_distillation} and gradient masking \cite{not_useful_gradients, obfuscated_gradients}. Recent work proposed a stateful defensive mechanism \cite{chen_stateful_2019} to accommodate the other schemes. Unlike previous defensive schemes, this scheme holds a state of previously submitted queries. This allows the scheme to make a decision based on a series of queries instead of a single query.\\

The stateful defense mechanism assumes that there is no collaboration between different users of the model and that every submitted query can be traced back to the submitting user. This is not necessarily the case in real scenarios, since users are free to work together. It is even possible for one user to set up multiple accounts and essentially collaborate with itself. This work aims to exploit the assumption made by the defensive mechanism and create adversarial examples while triggering as few detections as possible. While evasiveness of the attack might be the primary goal, the created adversarial example will have to closely resemble the original image to fool the human eye into believing they are identical. This resemblance is measured as the distance between the two images. The distance is in turn correlated to the efficiency of the attack as more efficient attacks yield lower distances.\\

This dual goal is achieved by first altering an existing adversarial attack using \gls{pso} in order to bypass the stateful defense mechanism. \gls{pso} is an optimization framework inspired by the flocking of birds. A swarm of candidate solutions, also called particles, moves through search space in an attempt to find the optimal position in this search space. The particles inform each other of their positions and the corresponding value of the optimization problem. The \gls{pso} framework is a viable candidate for this problem for several reasons. The swarm of particles will grant the attack to have multiple starting points, allowing it to be more efficient. The different starting positions have the additional benefit that the queries submitted to the model are more spaced out in the search space, causing the attack to be more evasive as well.\\ 

Afterwards multiple ideas are explored in an effort to improve the efficiency or evasiveness of the altered attack. Some of these ideas are specific to the created attack, while others are generally applicable to all other adversarial attacks.\\

The work is structured as follows: Chapter \ref{chap:background} discusses the necessary background needed in order to comprehend the rest of the work. Chapter \ref{chap:related_work} gives an overview of work related to this subject. Specific adversarial attacks are discussed as is the stateful defense mechanism. Chapter \ref{chap:approach} introduces the main ideas used in the development of the altered attack. This chapter also poses some interesting research questions. Chapter \ref{chap:evaluation} proposes a novel attack and iteratively refines this attack in order to increase the efficiency or evasiveness. At the end of every section of this chapter, conclusions are drawn and discussed. Chapter \ref{chap:discussion} contains a discussion about the work as a whole. Some final remarks are given as well as pointers for future work.  Finally, Chapter \ref{chap:conclusion} concludes this work by discussing the outcomes and insights of this text.

