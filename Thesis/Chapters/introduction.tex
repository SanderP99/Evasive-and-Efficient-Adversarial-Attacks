\chapter{Introduction}
% General classification (daunting)
% Neural networks
% DNN
% Adversarial attacks
% Defenses
% Bypass
% Overview
One of the most basic tasks in \gls{ai} or more specifically \gls{ml} is the classification of data. This task is usually performed by a classifier or model based on features present in this data. Some examples of classification tasks are: character recognition \cite{mnist}, spam detection \cite{spambase} and face recognition \cite{face_recognition_survey}. These tasks are generally very daunting for humans causing a great interest in improving the accuracies of these classifiers.\\

The first classifiers made use of algorithms such as nearest neighbors \cite{nearest_neighor}, decision trees \cite{decision_tree} and \glspl{svm} \cite{svm}. More recent classifiers are based on neural networks, which in turn are inspired by the human brain. These networks come in a great variety of forms and show near-human performance on some very specific tasks \cite{alpha_go_google}. This improvement in performance caused even more interest in the applications of \gls{ai} and \gls{ml}. Forbes \cite{forbes} recently predicted that in ten years \gls{ai} will be present in all areas of our society. The next decade is deemed as the \textit{"most promising era in technology innovation"} \cite{forbes}.\\

The prevalence of \gls{ml} in the near future does not only bring opportunities. It also poses some threats that may not be obvious at first glance. A spam email passing through an \gls{ai}-based spam filter might not be the end of the world, while mispredicting some characters from an image can have a bigger impact depending on the context. False predictions in the medical domain might be even more severe, but they could be overturned by a doctor. Self-driving cars not detecting traffic signs could put several lives at risk. Depending on the situation in which the classifier is deployed, some threats can be more dangerous than others.\\

All previous examples of threats are due to the inaccuracies of the classifier itself. There are also some threats caused by the inherent nature of neural networks. In 2014, it was shown that neural networks are prone to adversarial examples \cite{szegedy2014intriguing}. An adversarial example consists of a correctly classified data instance and a small perturbation. This perturbation causes the neural network to misclassify the instance. This allows malicious users to create images that are seemingly identical, while being classified differently. This poses an even greater threat to the security of \gls{ai} than the inaccuracies of the classifiers.\\

The process of creating an adversarial example is called an adversarial attack. Different types of attacks exist depending on the information available to the attacker. White-box attacks require complete knowledge of the classifier under attack, while black-box attacks only require the output(s) of the model. All attacks require sending one or more queries to the classifier.\\

Ever since the discovery of adversarial examples, an arms race between adversarial attack and defense researchers has been taken place. In the current state of affairs in the adversarial \gls{ml} domain, the stateful defensive mechanism \cite{chen_stateful_2019} is considered the state-of-the-art. Unlike previous defensive schemes, this scheme holds state of previously submitted queries. This allows the scheme to make a decision based on a series of queries instead of a single query.\\

The stateful defense mechanism makes the assumption that there is no collaboration between different users of the model and that every submitted query can be traced back to the submitting user. This is not necessarily the case in real scenarios, since users are free to work together. It is even possible for one user to set up multiple accounts and essentially collaborate with itself. This work aims to exploit the assumption made by the defensive mechanism and create adversarial examples while triggering as few detections as possible.\\

This goal is achieved by first altering an existing adversarial attack using \gls{pso} in order to bypass the stateful defense mechanism. \gls{pso} is an optimization framework inspired by the flocking of birds. Afterwards multiple ideas are explored in an effort to improve the efficiency or evasiveness of the altered attack. Some ideas are specific for the created attack, while others are generally applicable to all other adversarial attacks.\\

The work is structured as follows: Chapter \ref{chap:background} discusses the necessary background needed in order to comprehend the rest of the work. Chapter \ref{chap:related_work} gives an overview of work related to this subject. Some specific adversarial attacks are discussed as is the stateful defense mechanism. Chapter \ref{chap:approach} introduces the main ideas used in the development of the altered attack. This chapter also poses some interesting research questions. Chapter \ref{chap:evaluation} proposes a novel attack and iteratively refines this attack in order to increase the efficiency or evasiveness. At the end of every section of this chapter, some conclusions are drawn and discussed. Chapter \ref{chap:discussion} contains a discussion about the work as a whole. Some final remarks are given as well as some pointers for future work. Finally, Chapter \ref{chap:conclusion} concludes this work by answering the research questions posed in Chapter \ref{chap:approach}. \\ 