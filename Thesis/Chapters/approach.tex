\chapter{Approach}
The research concerning adversarial attacks and defenses is predominantly driven by a game of cat and mouse. Whenever a new attack is proposed, a defensive mechanism countering the novel attack is developed and vice versa. This work aims to create a new family of algorithms that can be used to perform both targeted and untargeted attacks. The goal of these algorithms is to craft adversarial examples comparable to state of the art approaches while remaining undetected by the stateful detection mechanism \cite{chen_stateful_2019} described in section \ref{sec:stateful_detection}.

\section{Distribution}
The stateful detection mechanism \cite{chen_stateful_2019} makes the assumption that queries can be traced back to their adversary and that there is no cooperation between different adversaries. This assumption can be problematic as $N$~collaborating adversaries can theoretically reduce the number of submitted queries per adversary by a factor~$1/N$. Even a single adversary could set up multiple accounts and submit queries on each account until it is banned. Due to the reduced number of submitted queries, less attacks will be detected since each buffer of the defense mechanism only holds a fraction of all queries.\\

This work will aim to evade the detection mechanism by distributing the query submissions over multiple nodes. Each node will represent a different user of the model under attack. The users could theoretically be all different persons or they could be different accounts of the same person.\\

As described in section \ref{sec:pso_and_distributed_attacks}, several attempts have been made to distribute adversarial attacks \cite{distributed_pso_attack, suryanto2020}. However, none of these attacks have been evaluated against the stateful detection mechanism, since the goal of the distribution was to make the attack more efficient in terms of the distance between the original image and the resulting adversarial example. This work will distribute the query submission over multiple nodes in order to avoid detection.\\

\section{Optimization} \label{sec:optimization_approach}
As previously mentioned, reducing the number of submitted queries per adversary by a factor~$1/N$, where $N$ is the number of collaborators, is straightforward. Adversaries can gain knowledge about the search space by cooperating with other adversaries. They can leverage this knowledge in order to reduce the number of submitted queries even more. This idea has been utilized by multiple algorithms that were mentioned in section \ref{sec:pso_and_distributed_attacks}. These algorithms used some form of \gls{pso} to optimize the final adversarial example. However all but the \gls{pso}-\gls{bba} algorithm by Xiang et al \cite{distributed_pso_attack} rely on the confidence score of the model for the fitness value calculation. All attacks discussed use \gls{pso} as an attack in itself. This work will combine the benefits of state of the art black box attacks and \gls{pso}.

\section{Approach}
The remaining sections of this work will propose a new family of adversarial attacks. First a threat model is defined in section \ref{sec:threat_model}. All remaining experiments will be performed with this threat model in mind. Afterwards the novel adversarial algorithm is proposed in section \ref{sec:combining_pso_bba}. This algorithm is iteratively improved based on the results of the experiments. Finally, the final algorithm will be compared with state of the art decision-based attacks.\\

During the process of creating, improving and optimizing the attack, this work tries to answer the following research questions. 
\begin{itemize}
	\item What are the (dis)advantages of using \gls{pso} in relation to vanilla adversarial attacks?
	\item How can \gls{pso} be combined with state of the art adversarial attacks?
	\item What are the (dis)advantages of distributing an adversarial attack?
\end{itemize} 

This work concludes with an answer on these questions in section \ref{chap:conclusion}.

\section{Threat model}\label{sec:threat_model}
This section will describe the threat model that will be used for the remainder of this work. The first and most impactful assumption is that the model under attack will only output labels for the input. There will be no confidence scores or model parameters available. The proposed attack will have to be a decision-based attack. Most real world \glspl{api} will only expose the final decision to the user, causing decision-based attacks to be the only viable option. Decision-based attacks are the most restricted type of attack. Therefore they can also be applied to score-based and white-box models.\\

The proposed attack will be a targeted attack, as it is the most relevant type from a security point of view. A targeted attack implementation can also easily be ported to an untargeted one. This is done by running a targeted attack for every possible class and selecting the one closest to the original image.\\

The model will be defended by by a stateful detection mechanism \cite{chen_stateful_2019} as described in section \ref{sec:stateful_detection}. It will have a query bounded buffer for each account. Once an query has been flagged as potential attack, the account that submitted the query will be banned. The user will have to set up a fresh account in order to submit queries again.\\

There are two main fees associated with the model. The first fee is related to setting up an account. The assumption is made that setting up an account requires a valid credit card or phone number. Whenever an account is banned, the credit card or phone number is invalidated in the system, requiring the attacker to sign up for a new credit card or register for a new phone number. This cost is identical to the assumption made by Chen et al \cite{chen_stateful_2019}. The second fee is related to the amount of queries submitted to the model. The more queries an attacker submits, the higher the total cost will be. Chen et al \cite{chen_stateful_2019} did not incorporate a cost per query, but most vision \glspl{api}, such as Google Cloud Vision \cite{google_pricing} and Amazon Rekognition \cite{amazon_pricing} use this type of fee.\\

These fees ensure that attackers need to be both efficient and evasive in order to be successful. The evasiveness of the attack is correlated to the number of detections by the stateful defense mechanism. Each detection essentially means that the attacker has to set up a new account and pay the associated cost. To compare the efficiency of different attacks, the following strategy will be used. Every run of an attack has a budget of queries. The attack that has created an adversarial example closest to the original image inside this budget of queries, is the most efficient. The evasiveness of an attack is the primary concern, since the cost of setting up a new account is significantly higher than the cost per query.

